{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from functools import partial\n",
    "from modelling_finetune import get_vit_config, LongViTForClassification\n",
    "from models.cmil import CMILModel, FeatureExtractor, SliceFusionTransformer\n",
    "\n",
    "device = torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of patches: 12288\n",
      "Using Torchscale LongNetEncoder\n",
      "torch.Size([4, 14])\n"
     ]
    }
   ],
   "source": [
    "config = get_vit_config(img_size=(384,512,512), patch_size=(8,32,32), embed_dim=384, depth=12, num_heads=16,\n",
    "                        norm_layer=partial(nn.LayerNorm, eps=1e-6))\n",
    "\n",
    "v = LongViTForClassification(config, num_classes=14).to(device).half()\n",
    "\n",
    "ct = torch.randn(4, 1, 384, 512, 512, device=device).half()\n",
    "preds = v(ct)\n",
    "print(preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0820, -0.2520,  0.3376, -0.1135, -0.1403, -0.0159, -0.0265,  0.1301,\n",
       "         -0.2076, -0.1617, -0.1147, -0.1781,  0.2322, -0.3887],\n",
       "        [-0.0188,  0.1843,  0.0089, -0.0826, -0.3115, -0.0507,  0.2876,  0.3669,\n",
       "         -0.0900, -0.0640, -0.1317, -0.0127,  0.1244,  0.1436],\n",
       "        [ 0.0865,  0.5117, -0.0729, -0.4053, -0.0200,  0.2365,  0.2277, -0.2998,\n",
       "          0.1593, -0.0018, -0.2058, -0.4873,  0.0013,  0.0353],\n",
       "        [ 0.3340,  0.3708, -0.0269,  0.0812, -0.4121, -0.0415,  0.1624,  0.0815,\n",
       "         -0.2766,  0.1267,  0.0272,  0.1121,  0.3857,  0.0342]],\n",
       "       device='cuda:2', dtype=torch.float16, grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/than/anaconda3/envs/longvit/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "Using cache found in /home/than/.cache/torch/hub/facebookresearch_dinov2_main\n",
      "/home/than/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is not available (SwiGLU)\")\n",
      "/home/than/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:33: UserWarning: xFormers is not available (Attention)\n",
      "  warnings.warn(\"xFormers is not available (Attention)\")\n",
      "/home/than/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:40: UserWarning: xFormers is not available (Block)\n",
      "  warnings.warn(\"xFormers is not available (Block)\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 384])\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 384  # Must match the embed_dim in FeatureExtractor (DINOv2 output)\n",
    "num_heads = 16\n",
    "hidden_dim = 2048\n",
    "num_layers = 1\n",
    "patch_size = 1  # Patch size for the SliceFusionTransformer\n",
    "\n",
    "max_seq_len = 256  # Adjust based on your data\n",
    "transformer_model = SliceFusionTransformer(\n",
    "    seq_len=max_seq_len,\n",
    "    embed_dim=embed_dim,\n",
    "    num_heads=num_heads,\n",
    "    hidden_dim=hidden_dim,\n",
    "    num_layers=num_layers,\n",
    "    patch_size=patch_size\n",
    ")\n",
    "\n",
    "model = CMILModel(FeatureExtractor(model_name='dinov2_vits14'), transformer_model).to(device).half()\n",
    "\n",
    "ct = torch.randn(4, 64, 3, 224, 224, device=device).half()\n",
    "preds = model(ct)\n",
    "print(preds.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "longvit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
