{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from functools import partial\n",
    "from modelling_finetune import get_vit_config, LongViTForClassification\n",
    "from models.cmil import CMILModel, FeatureExtractor, SliceFusionTransformer\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of patches: 4096\n",
      "Using Torchscale LongNetEncoder\n",
      "torch.Size([4, 14])\n"
     ]
    }
   ],
   "source": [
    "config = get_vit_config(img_size=(64,256,256), patch_size=(4,16,16), embed_dim=384, depth=12, num_heads=16,\n",
    "                        norm_layer=partial(nn.LayerNorm, eps=1e-6))\n",
    "\n",
    "v = LongViTForClassification(config, num_classes=14).to(device).half()\n",
    "\n",
    "ct = torch.randn(4, 1, 64, 256, 256, device=device).half()\n",
    "preds = v(ct)\n",
    "print(preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4096"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.model.patch_embed.num_patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0527, -0.0679,  0.0416, -0.4260, -0.0988,  0.2468, -0.0457, -0.1605,\n",
       "          0.0790, -0.0021, -0.3213, -0.0792,  0.1904,  0.1860],\n",
       "        [ 0.2742, -0.1193,  0.2974, -0.0947,  0.1022,  0.1819, -0.1227, -0.1455,\n",
       "         -0.0143,  0.2477, -0.2988,  0.4233, -0.0638,  0.2346],\n",
       "        [-0.1184, -0.0112,  0.1346, -0.1506,  0.0205,  0.2908,  0.0127,  0.3157,\n",
       "          0.0383, -0.0862,  0.0596, -0.0310,  0.0005, -0.0715],\n",
       "        [ 0.0455,  0.1278,  0.1597, -0.0884, -0.0680,  0.0740, -0.0955, -0.1681,\n",
       "         -0.2808,  0.0016, -0.0254, -0.0112, -0.0260, -0.1202]],\n",
       "       device='cuda:0', dtype=torch.float16, grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/than/anaconda3/envs/longvit/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "Using cache found in /home/than/.cache/torch/hub/facebookresearch_dinov2_main\n",
      "/home/than/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is not available (SwiGLU)\")\n",
      "/home/than/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:33: UserWarning: xFormers is not available (Attention)\n",
      "  warnings.warn(\"xFormers is not available (Attention)\")\n",
      "/home/than/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:40: UserWarning: xFormers is not available (Block)\n",
      "  warnings.warn(\"xFormers is not available (Block)\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 384])\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 384  # Must match the embed_dim in FeatureExtractor (DINOv2 output)\n",
    "num_heads = 16\n",
    "hidden_dim = 2048\n",
    "num_layers = 1\n",
    "patch_size = 1  # Patch size for the SliceFusionTransformer\n",
    "\n",
    "max_seq_len = 256  # Adjust based on your data\n",
    "transformer_model = SliceFusionTransformer(\n",
    "    seq_len=max_seq_len,\n",
    "    embed_dim=embed_dim,\n",
    "    num_heads=num_heads,\n",
    "    hidden_dim=hidden_dim,\n",
    "    num_layers=num_layers,\n",
    "    patch_size=patch_size\n",
    ")\n",
    "\n",
    "model = CMILModel(FeatureExtractor(model_name='dinov2_vits14'), transformer_model).to(device).half()\n",
    "\n",
    "ct = torch.randn(4, 64, 3, 224, 224, device=device).half()\n",
    "preds = model(ct)\n",
    "print(preds.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "longvit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
