{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import lmdb\n",
    "import pickle\n",
    "import ast\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "from models import models_vit\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from matplotlib import pyplot as plt\n",
    "from util.pos_embed import interpolate_pos_embed\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of patches: 43264\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')\n",
    "v = models_vit.VisionTransformer(18, img_size=(256, 416, 416), num_heads=16, \n",
    "                                     segment_length=\"[2704, 5408, 10816, 21632, 43264]\",)\n",
    "moddel = v.to(device).half()\n",
    "ct = torch.randn(1, 1, 256, 416, 416, device=device).half()\n",
    "preds = v(ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 18])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6489,  0.2847, -0.4468, -0.3201,  0.9175, -0.2333, -0.3855, -0.1503,\n",
       "          0.0775, -0.2583,  0.0063, -0.2595,  0.2935, -0.8589, -0.7666, -0.1348,\n",
       "         -0.4915, -0.0210]], device='cuda:2', dtype=torch.float16,\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.shape\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load pre-trained checkpoint from: checkpoints/checkpoint-799.pth\n",
      "Number of patches: 98304\n",
      "Position interpolate from 32x14x14 to 96x32x32\n",
      "_IncompatibleKeys(missing_keys=['head.weight', 'head.bias'], unexpected_keys=['mask_token', 'decoder_pos_embed', 'decoder_embed.weight', 'decoder_embed.bias', 'decoder.layers.0.self_attn.k_proj.weight', 'decoder.layers.0.self_attn.k_proj.bias', 'decoder.layers.0.self_attn.v_proj.weight', 'decoder.layers.0.self_attn.v_proj.bias', 'decoder.layers.0.self_attn.q_proj.weight', 'decoder.layers.0.self_attn.q_proj.bias', 'decoder.layers.0.self_attn.out_proj.weight', 'decoder.layers.0.self_attn.out_proj.bias', 'decoder.layers.0.self_attn.inner_attn_ln.weight', 'decoder.layers.0.self_attn.inner_attn_ln.bias', 'decoder.layers.0.self_attn_layer_norm.weight', 'decoder.layers.0.self_attn_layer_norm.bias', 'decoder.layers.0.ffn.fc1.weight', 'decoder.layers.0.ffn.fc1.bias', 'decoder.layers.0.ffn.fc2.weight', 'decoder.layers.0.ffn.fc2.bias', 'decoder.layers.0.ffn.ffn_layernorm.weight', 'decoder.layers.0.ffn.ffn_layernorm.bias', 'decoder.layers.0.final_layer_norm.weight', 'decoder.layers.0.final_layer_norm.bias', 'decoder.layers.1.self_attn.k_proj.weight', 'decoder.layers.1.self_attn.k_proj.bias', 'decoder.layers.1.self_attn.v_proj.weight', 'decoder.layers.1.self_attn.v_proj.bias', 'decoder.layers.1.self_attn.q_proj.weight', 'decoder.layers.1.self_attn.q_proj.bias', 'decoder.layers.1.self_attn.out_proj.weight', 'decoder.layers.1.self_attn.out_proj.bias', 'decoder.layers.1.self_attn.inner_attn_ln.weight', 'decoder.layers.1.self_attn.inner_attn_ln.bias', 'decoder.layers.1.self_attn_layer_norm.weight', 'decoder.layers.1.self_attn_layer_norm.bias', 'decoder.layers.1.ffn.fc1.weight', 'decoder.layers.1.ffn.fc1.bias', 'decoder.layers.1.ffn.fc2.weight', 'decoder.layers.1.ffn.fc2.bias', 'decoder.layers.1.ffn.ffn_layernorm.weight', 'decoder.layers.1.ffn.ffn_layernorm.bias', 'decoder.layers.1.final_layer_norm.weight', 'decoder.layers.1.final_layer_norm.bias', 'decoder.layers.2.self_attn.k_proj.weight', 'decoder.layers.2.self_attn.k_proj.bias', 'decoder.layers.2.self_attn.v_proj.weight', 'decoder.layers.2.self_attn.v_proj.bias', 'decoder.layers.2.self_attn.q_proj.weight', 'decoder.layers.2.self_attn.q_proj.bias', 'decoder.layers.2.self_attn.out_proj.weight', 'decoder.layers.2.self_attn.out_proj.bias', 'decoder.layers.2.self_attn.inner_attn_ln.weight', 'decoder.layers.2.self_attn.inner_attn_ln.bias', 'decoder.layers.2.self_attn_layer_norm.weight', 'decoder.layers.2.self_attn_layer_norm.bias', 'decoder.layers.2.ffn.fc1.weight', 'decoder.layers.2.ffn.fc1.bias', 'decoder.layers.2.ffn.fc2.weight', 'decoder.layers.2.ffn.fc2.bias', 'decoder.layers.2.ffn.ffn_layernorm.weight', 'decoder.layers.2.ffn.ffn_layernorm.bias', 'decoder.layers.2.final_layer_norm.weight', 'decoder.layers.2.final_layer_norm.bias', 'decoder.layers.3.self_attn.k_proj.weight', 'decoder.layers.3.self_attn.k_proj.bias', 'decoder.layers.3.self_attn.v_proj.weight', 'decoder.layers.3.self_attn.v_proj.bias', 'decoder.layers.3.self_attn.q_proj.weight', 'decoder.layers.3.self_attn.q_proj.bias', 'decoder.layers.3.self_attn.out_proj.weight', 'decoder.layers.3.self_attn.out_proj.bias', 'decoder.layers.3.self_attn.inner_attn_ln.weight', 'decoder.layers.3.self_attn.inner_attn_ln.bias', 'decoder.layers.3.self_attn_layer_norm.weight', 'decoder.layers.3.self_attn_layer_norm.bias', 'decoder.layers.3.ffn.fc1.weight', 'decoder.layers.3.ffn.fc1.bias', 'decoder.layers.3.ffn.fc2.weight', 'decoder.layers.3.ffn.fc2.bias', 'decoder.layers.3.ffn.ffn_layernorm.weight', 'decoder.layers.3.ffn.ffn_layernorm.bias', 'decoder.layers.3.final_layer_norm.weight', 'decoder.layers.3.final_layer_norm.bias', 'decoder.layers.4.self_attn.k_proj.weight', 'decoder.layers.4.self_attn.k_proj.bias', 'decoder.layers.4.self_attn.v_proj.weight', 'decoder.layers.4.self_attn.v_proj.bias', 'decoder.layers.4.self_attn.q_proj.weight', 'decoder.layers.4.self_attn.q_proj.bias', 'decoder.layers.4.self_attn.out_proj.weight', 'decoder.layers.4.self_attn.out_proj.bias', 'decoder.layers.4.self_attn.inner_attn_ln.weight', 'decoder.layers.4.self_attn.inner_attn_ln.bias', 'decoder.layers.4.self_attn_layer_norm.weight', 'decoder.layers.4.self_attn_layer_norm.bias', 'decoder.layers.4.ffn.fc1.weight', 'decoder.layers.4.ffn.fc1.bias', 'decoder.layers.4.ffn.fc2.weight', 'decoder.layers.4.ffn.fc2.bias', 'decoder.layers.4.ffn.ffn_layernorm.weight', 'decoder.layers.4.ffn.ffn_layernorm.bias', 'decoder.layers.4.final_layer_norm.weight', 'decoder.layers.4.final_layer_norm.bias', 'decoder.layers.5.self_attn.k_proj.weight', 'decoder.layers.5.self_attn.k_proj.bias', 'decoder.layers.5.self_attn.v_proj.weight', 'decoder.layers.5.self_attn.v_proj.bias', 'decoder.layers.5.self_attn.q_proj.weight', 'decoder.layers.5.self_attn.q_proj.bias', 'decoder.layers.5.self_attn.out_proj.weight', 'decoder.layers.5.self_attn.out_proj.bias', 'decoder.layers.5.self_attn.inner_attn_ln.weight', 'decoder.layers.5.self_attn.inner_attn_ln.bias', 'decoder.layers.5.self_attn_layer_norm.weight', 'decoder.layers.5.self_attn_layer_norm.bias', 'decoder.layers.5.ffn.fc1.weight', 'decoder.layers.5.ffn.fc1.bias', 'decoder.layers.5.ffn.fc2.weight', 'decoder.layers.5.ffn.fc2.bias', 'decoder.layers.5.ffn.ffn_layernorm.weight', 'decoder.layers.5.ffn.ffn_layernorm.bias', 'decoder.layers.5.final_layer_norm.weight', 'decoder.layers.5.final_layer_norm.bias', 'decoder.layers.6.self_attn.k_proj.weight', 'decoder.layers.6.self_attn.k_proj.bias', 'decoder.layers.6.self_attn.v_proj.weight', 'decoder.layers.6.self_attn.v_proj.bias', 'decoder.layers.6.self_attn.q_proj.weight', 'decoder.layers.6.self_attn.q_proj.bias', 'decoder.layers.6.self_attn.out_proj.weight', 'decoder.layers.6.self_attn.out_proj.bias', 'decoder.layers.6.self_attn.inner_attn_ln.weight', 'decoder.layers.6.self_attn.inner_attn_ln.bias', 'decoder.layers.6.self_attn_layer_norm.weight', 'decoder.layers.6.self_attn_layer_norm.bias', 'decoder.layers.6.ffn.fc1.weight', 'decoder.layers.6.ffn.fc1.bias', 'decoder.layers.6.ffn.fc2.weight', 'decoder.layers.6.ffn.fc2.bias', 'decoder.layers.6.ffn.ffn_layernorm.weight', 'decoder.layers.6.ffn.ffn_layernorm.bias', 'decoder.layers.6.final_layer_norm.weight', 'decoder.layers.6.final_layer_norm.bias', 'decoder.layers.7.self_attn.k_proj.weight', 'decoder.layers.7.self_attn.k_proj.bias', 'decoder.layers.7.self_attn.v_proj.weight', 'decoder.layers.7.self_attn.v_proj.bias', 'decoder.layers.7.self_attn.q_proj.weight', 'decoder.layers.7.self_attn.q_proj.bias', 'decoder.layers.7.self_attn.out_proj.weight', 'decoder.layers.7.self_attn.out_proj.bias', 'decoder.layers.7.self_attn.inner_attn_ln.weight', 'decoder.layers.7.self_attn.inner_attn_ln.bias', 'decoder.layers.7.self_attn_layer_norm.weight', 'decoder.layers.7.self_attn_layer_norm.bias', 'decoder.layers.7.ffn.fc1.weight', 'decoder.layers.7.ffn.fc1.bias', 'decoder.layers.7.ffn.fc2.weight', 'decoder.layers.7.ffn.fc2.bias', 'decoder.layers.7.ffn.ffn_layernorm.weight', 'decoder.layers.7.ffn.ffn_layernorm.bias', 'decoder.layers.7.final_layer_norm.weight', 'decoder.layers.7.final_layer_norm.bias', 'decoder_norm.weight', 'decoder_norm.bias', 'decoder_pred.weight', 'decoder_pred.bias'])\n"
     ]
    }
   ],
   "source": [
    "finetune = 'checkpoints/checkpoint-799.pth'\n",
    "\n",
    "checkpoint = torch.load(finetune, map_location='cpu')\n",
    "\n",
    "print(\"Load pre-trained checkpoint from: %s\" % finetune)\n",
    "checkpoint_model = checkpoint['model']\n",
    "model = models_vit.__dict__['vit_base_patch16'](\n",
    "        num_classes=18,\n",
    "    )\n",
    "state_dict = model.state_dict()\n",
    "for k in ['head.weight', 'head.bias']:\n",
    "    if k in checkpoint_model and checkpoint_model[k].shape != state_dict[k].shape:\n",
    "        print(f\"Removing key {k} from pretrained checkpoint\")\n",
    "        del checkpoint_model[k]\n",
    "\n",
    "# interpolate position embedding\n",
    "interpolate_pos_embed(model, checkpoint_model)\n",
    "\n",
    "# load pre-trained model\n",
    "msg = model.load_state_dict(checkpoint_model, strict=False)\n",
    "print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'head.bias', 'head.weight'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(msg.missing_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.half()\n",
    "out = model(torch.randn(1, 1, 384, 512, 512).half())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "longvit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
